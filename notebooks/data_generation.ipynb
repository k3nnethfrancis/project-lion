{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kencav/project-lion/blob/master/notebooks/data_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:\n",
        "\n",
        "\n",
        "\n",
        "*  Modularize data generation\n",
        "    * Structure data-gen class\n",
        "*  Expand dataset selection\n",
        "    * Enron dataset\n",
        "    * Discord scraper\n",
        "    * Stanford paper (add link to this)\n",
        "\n"
      ],
      "metadata": {
        "id": "0-sYrq3PaGCq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "XKTbQ_YxTjvP",
        "outputId": "06b39c07-ca1c-40e0-e72c-b6f6a59d73f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.7/dist-packages (0.11.5)\n",
            "Requirement already satisfied: pandas-stubs>=1.1.0.11 in /usr/local/lib/python3.7/dist-packages (from openai) (1.2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai) (4.62.3)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from openai) (3.0.9)\n",
            "Collecting pandas>=1.2.3\n",
            "  Using cached pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pandas-stubs>=1.1.0.11->openai) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.0\n",
            "    Uninstalling pandas-1.1.0:\n",
            "      Successfully uninstalled pandas-1.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.5 which is incompatible.\u001b[0m\n",
            "Successfully installed pandas-1.3.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.1.0\n",
            "  Using cached pandas-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.0) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.0) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.0) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "openai 0.11.5 requires pandas>=1.2.3, but you have pandas 1.1.0 which is incompatible.\u001b[0m\n",
            "Successfully installed pandas-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# INSTALL GENERAL DEPENDENCIES\n",
        "\n",
        "#! python3\n",
        "!pip install openai\n",
        "# Downgrade pandas to be compatible with Colab\n",
        "!pip install pandas==1.1.0\n",
        "\n",
        "#libraries\n",
        "import os\n",
        "import openai\n",
        "\n",
        "import datetime\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CONNECT KAGGLE\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q kaggle\n",
        "!pip install -q kaggle-cli\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp \"/content/drive/MyDrive/kaggle.json\" ~/.kaggle/\n",
        "!cat ~/.kaggle/kaggle.json \n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Format of kaggle download command (non-competition datasets). ENRON EMAILS DATASET CRASHES JESSIE'S LAPTOP\n",
        "#!kaggle datasets download -d wcukierski/enron-email-dataset -p enron_emails\n",
        "\n",
        "# SAMPLE CODE; DESIRED DATASET TOO BIG\n",
        "\n",
        "# Alternate way to download kaggle files\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "# Download all\n",
        "api.dataset_download_files('imdevskp/corona-virus-report')\n",
        "\n",
        "# Download 1\n",
        "api.dataset_download_file('imdevskp/corona-virus-report','covid_19_clean_complete.csv')\n",
        "\n",
        "zf = ZipFile('covid_19_clean_complete.csv.zip')\n",
        "#extracted data is saved in the same directory as notebook\n",
        "zf.extractall() \n",
        "zf.close()\n",
        "\n",
        "import pandas as pd\n",
        "data=pd.read_csv('covid_19_clean_complete.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfMulLHYagFQ",
        "outputId": "5481c9b0-a215-40bf-fdab-376442dc7cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "  Building wheel for lxml (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for lxml\u001b[0m\n",
            "\u001b[?25h    Running setup.py install for lxml ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-imsnh6e8/lxml_36195e738bda45629f305335db51ffa6/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-imsnh6e8/lxml_36195e738bda45629f305335db51ffa6/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-vz0cg00a/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/lxml Check the logs for full command output.\u001b[0m\n",
            "{\"username\":\"apersonofnote\",\"key\":\"8fb81f69e92f3607af52bd6c6b7c4fd8\"}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CONNECT TWITTER \n",
        "\n",
        "#! python3\n",
        "\n",
        "#libraries\n",
        "import pandas as pd\n",
        "import tweepy\n",
        "import csv\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "#Twitter API credentials\n",
        "consumer_key = 'CcoYGfKzL7Hgmk4d2K0F8p7ff'\n",
        "consumer_secret = 'lBGQo8qYEY8UxPxljSASvEbY6BwTycCAstmQgVkTqmNF8iGtbK'\n",
        "access_key = '4600137193-q3fORQZDy4KproA7hv4hTGtZ6PjLF7fJUboH20x'\n",
        "access_secret = 'jBQMNfZq9WkSQT2O8kAV79f9BKtuPtYc5WlpQRhe5WWK4'\n",
        "\n",
        "\n",
        "API_KEY = \"Y0iyPQIvdeAAUWgzvIJGFOApS\"\n",
        "\n",
        "\n",
        "\n",
        "def validate_twitter(consumer_key, consumer_secret, access_key, access_secret):\n",
        "  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "  auth.set_access_token(access_key, access_secret)\n",
        "\n",
        "  api = tweepy.API(auth)\n",
        "\n",
        "  return api\n",
        "\n",
        "def sort_tweets_by_time_chunk(input_df, column_name='Date', time_chunk=\"week\"):\n",
        "    for x in range(52):\n",
        "      df = input_df[input_df[column_name].dt.isocalendar().week == x]\n",
        "      print(df.count)\n",
        "      # Only make folders and files for weeks where there were tweets\n",
        "      if not df.empty:\n",
        "        if not os.path.exists(f'week-{x}'):\n",
        "          os.makedirs(f'week-{x}')\n",
        "        df.to_csv(f'week-{x}/{screen_name}.csv')\n",
        "\n",
        "# get an arbitrary # of tweets\n",
        "def get_tweets(screen_name, num, api):\n",
        "    new_tweets = api.user_timeline(screen_name=screen_name, count=num)\n",
        "    \n",
        "    return new_tweets\n",
        "\n",
        "\n",
        "def get_all_tweets(screen_name, api):\n",
        "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
        "\n",
        "    #initialize a list to hold all the tweepy Tweets\n",
        "    alltweets = []\n",
        "\n",
        "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
        "    new_tweets = api.user_timeline(screen_name=screen_name, count=200)\n",
        "\n",
        "    #save most recent tweets\n",
        "    alltweets.extend(new_tweets)\n",
        "\n",
        "    #save the id of the oldest tweet less one\n",
        "    oldest = alltweets[-1].id - 1\n",
        "\n",
        "    #keep grabbing tweets until there are no tweets left to grab\n",
        "    while len(new_tweets) > 0:\n",
        "        print(f\"getting tweets before {oldest}\")\n",
        "\n",
        "        #all subsiquent requests use the max_id param to prevent duplicates\n",
        "        new_tweets = api.user_timeline(screen_name=screen_name,\n",
        "                                       count=200,\n",
        "                                       max_id=oldest)\n",
        "\n",
        "        #save most recent tweets\n",
        "        alltweets.extend(new_tweets)\n",
        "\n",
        "        #update the id of the oldest tweet less one\n",
        "        oldest = alltweets[-1].id - 1\n",
        "\n",
        "        print(f\"...{len(alltweets)} tweets downloaded so far\")\n",
        "\n",
        "    #transform the tweepy tweets into a 2D array that will populate the csv\n",
        "    outtweets = [[tweet.id_str, tweet.created_at, tweet.text]\n",
        "                 for tweet in alltweets]\n",
        "                 \n",
        "    # Transposed in the repl.it\n",
        "    tweets_df = pd.DataFrame(outtweets,\n",
        "                             columns=[\"Tweet_id\", \"Date\", \"Tweet_Text\"])\n",
        "    \n",
        "    # TODO: incorporate this start to error handling\n",
        "    # except BaseException as e:\n",
        "    #       print('failed on_status', str(e))\n",
        "    #       time.sleep(3)\n",
        "\n",
        "  # For each week in the year, create dir if ! exists, filter df, write to csv, and save to folder\n",
        "  # See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.weekofyear.html  \n",
        "    '''\n",
        "    for x in range(52):\n",
        "      df = tweets_df[tweets_df['Date'].dt.isocalendar().week == x]\n",
        "      print(df.count)\n",
        "      # Only make folders and files for weeks where there were tweets\n",
        "      if not df.empty:\n",
        "        if not os.path.exists(f'week-{x}'):\n",
        "          os.makedirs(f'week-{x}')\n",
        "        df.to_csv(f'week-{x}/{screen_name}.csv')\n",
        "    '''\n",
        "    return tweets_df\n",
        "\n",
        "    \n",
        "# If using datetime object, can use this function to get the week for an individual tweet\n",
        "# Note that every python data type handles this slightly differently! Check the type!\n",
        "def make_chunks(data, tuple_position=1):\n",
        "  # 0 = year, 1 = week, 2 = day\n",
        "  weeks = []\n",
        "  for x in data:\n",
        "    chunk = x.created_at.isocalendar()[tuple_position]\n",
        "    weeks.append(chunk)\n",
        "  return weeks\n",
        "\n",
        "# #tweets\n",
        "# tweets = pd.read_csv('bored_tweets.csv')\n",
        "\n",
        "# #remove links\n",
        "# no_links = tweets[~tweets.text.str.contains(\"http\")]\n",
        "# no_links.head()\n",
        "\n",
        "# # only tweets\n",
        "# just_tweets = no_links.text\n",
        "\n",
        "# # make everything a string\n",
        "# text = str(just_tweets)\n",
        "\n",
        "\n",
        "api = validate_twitter(consumer_key, consumer_secret, access_key, access_secret)\n",
        "test = get_all_tweets('BoredElonMusk', api)\n",
        "print(test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cluIuLnyUP1C",
        "outputId": "3eb2ea75-07c1-40e8-d5b2-df1d513b5d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting tweets before 1476076237432901632\n",
            "...400 tweets downloaded so far\n",
            "getting tweets before 1472332160543559681\n",
            "...600 tweets downloaded so far\n",
            "getting tweets before 1468350290092908544\n",
            "...800 tweets downloaded so far\n",
            "getting tweets before 1461187944123748357\n",
            "...1000 tweets downloaded so far\n",
            "getting tweets before 1456288787831537665\n",
            "...1200 tweets downloaded so far\n",
            "getting tweets before 1453461088251506692\n",
            "...1400 tweets downloaded so far\n",
            "getting tweets before 1445819076073328644\n",
            "...1600 tweets downloaded so far\n",
            "getting tweets before 1442996049262493697\n",
            "...1800 tweets downloaded so far\n",
            "getting tweets before 1439633102150635522\n",
            "...2000 tweets downloaded so far\n",
            "getting tweets before 1436130768128917531\n",
            "...2200 tweets downloaded so far\n",
            "getting tweets before 1433763634119729154\n",
            "...2400 tweets downloaded so far\n",
            "getting tweets before 1430462321915088896\n",
            "...2600 tweets downloaded so far\n",
            "getting tweets before 1427079018990374914\n",
            "...2800 tweets downloaded so far\n",
            "getting tweets before 1420841772696031233\n",
            "...3000 tweets downloaded so far\n",
            "getting tweets before 1411116267851845631\n",
            "...3200 tweets downloaded so far\n",
            "getting tweets before 1407423256060063748\n",
            "...3250 tweets downloaded so far\n",
            "getting tweets before 1405977383983669249\n",
            "...3250 tweets downloaded so far\n",
            "                 Tweet_id  ...                                         Tweet_Text\n",
            "0     1478508067213885440  ...  @jack A lot of energy is spent (wasted) debati...\n",
            "1     1478504331057852417  ...                               @griillz_eth I'm in.\n",
            "2     1478502749281001472  ...  @mattmireles @john_c_palmer @makinmarkets Ange...\n",
            "3     1478455370481766400  ...  @jamie247 I'd like to learn more but first nee...\n",
            "4     1478445682075729923  ...  @brianjcho @amytongwu Cosign on those and if y...\n",
            "...                   ...  ...                                                ...\n",
            "3245  1406296934852161549  ...                                   @sriramk Welcome\n",
            "3246  1406240299442065408  ...         @fvckrender @Ioannis_AG @Sothebys Congrats\n",
            "3247  1406226324318945282  ...  @PatrickWStanley Maybe all the BTC accumulatio...\n",
            "3248  1405982338123587588  ...  @JanetWuNews Optional steam release at the cho...\n",
            "3249  1405977383983669250  ...  Washer and dryer units that play quiet techno ...\n",
            "\n",
            "[3250 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CONNECT DISCORD #\n",
        "\n",
        "import requests\n",
        "\n",
        "headers = {\"Authorization\": f\"Bot {bot_token}\"}\n",
        "\n",
        "test_channel_id = 885948250253848588\n",
        "test_guild_id = 647140436686667776\n",
        "\n",
        "channel_path = f'https://discord.com/api/channels/{test_channel_id}/messages'\n",
        "\n",
        "guild_path_channels = f'https://discord.com/api/guilds/{test_guild_id}/channels'\n",
        "\n",
        "\n",
        "res = requests.get(channel_path, headers=headers).json()\n",
        "\n",
        "for x in res:\n",
        "  print(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "f3UvncJa3w3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SUPPORT FUNCTIONS #\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "# Lazy util function\n",
        "def check_type(data):\n",
        "  print(type(data))\n",
        "\n",
        "def NUKE_ALL_FOLDERS_AND_FILES():\n",
        "  print(\"Nuclear launch detected\")\n",
        "  # Get directory name\n",
        "  # TODO: Update this to be more flexible, but it works for now\n",
        "  for x in range(1,52):\n",
        "    try:\n",
        "        shutil.rmtree(f'week-{x}')\n",
        "    except OSError as e:\n",
        "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
        "\n",
        "#NUKE_ALL_FOLDERS_AND_FILES()\n",
        "\n",
        "#os.listdir()"
      ],
      "metadata": {
        "id": "et_1gAPgUWLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TODO: Objectify functions\n",
        "\n",
        "def generate_df(input_df, file_path_name):\n",
        "  '''\n",
        "  - For x in df, grab x['date'] and split into day/month/year/week of year\n",
        "  - Make columns for each val and assign accordingly\n",
        "  - This will result in one large spreadsheet that you can filter\n",
        "  '''\n",
        "  pass\n",
        "\n",
        "def parse_by_weeks(input_df, file_path_name):\n",
        "    for x in range(52):\n",
        "      df = input_df[input_df['Date'].dt.isocalendar().week == x]\n",
        "      print(df.count)\n",
        "      # Only make folders and files for weeks where there were tweets\n",
        "      if not df.empty:\n",
        "        if not os.path.exists(f'week-{x}'):\n",
        "          os.makedirs(f'week-{x}')\n",
        "        df.to_csv(f'week-{x}/{file_path_name}.csv')\n",
        "    return input_df\n",
        "\n",
        "  \n",
        "generate_data(test, \"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LllFBPZDdN5u",
        "outputId": "bb405c9c-cb1c-4f7a-fb93-f00628197102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of                Tweet_id  ...                                         Tweet_Text\n",
            "0   1478508067213885440  ...  @jack A lot of energy is spent (wasted) debati...\n",
            "1   1478504331057852417  ...                               @griillz_eth I'm in.\n",
            "2   1478502749281001472  ...  @mattmireles @john_c_palmer @makinmarkets Ange...\n",
            "3   1478455370481766400  ...  @jamie247 I'd like to learn more but first nee...\n",
            "4   1478445682075729923  ...  @brianjcho @amytongwu Cosign on those and if y...\n",
            "..                  ...  ...                                                ...\n",
            "70  1477839129744314371  ...  @cobie @EFillae @CryptOH10 @Comyk34 @LeCryptog...\n",
            "71  1477824055835168774  ...  @bchesky @levie I’m still salty Aaron’s compan...\n",
            "72  1477817531159101444  ...                                @austin_rief Savage\n",
            "73  1477817222500274176  ...  @TheChainGamer @GuildOfGuardian @AxieInfinity ...\n",
            "74  1477812543049457665  ...             @Zeneca_33 The upside is astronomical.\n",
            "\n",
            "[75 rows x 3 columns]>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of Empty DataFrame\n",
            "Columns: [Tweet_id, Date, Tweet_Text]\n",
            "Index: []>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "3238  1406666376136200195  ...  @CryptoChris91 I can see why you’re interested :)\n",
            "3239  1406394178678915074  ...                       .@Sothebys Question for you.\n",
            "3240  1406344786664062977  ...  @robsaker The rescue shelter industrial comple...\n",
            "3241  1406341112596373506  ...  A rescue dog shelter that offers free dogs, a ...\n",
            "3242  1406320804128694272  ...                         @akbirthko Not on the app.\n",
            "3243  1406319955998511105  ...      @Quicktake @iSimy The first that YOU know of.\n",
            "3244  1406319572379078656  ...  I would pay to not have Twitter trends or an e...\n",
            "3245  1406296934852161549  ...                                   @sriramk Welcome\n",
            "3246  1406240299442065408  ...         @fvckrender @Ioannis_AG @Sothebys Congrats\n",
            "3247  1406226324318945282  ...  @PatrickWStanley Maybe all the BTC accumulatio...\n",
            "3248  1405982338123587588  ...  @JanetWuNews Optional steam release at the cho...\n",
            "3249  1405977383983669250  ...  Washer and dryer units that play quiet techno ...\n",
            "\n",
            "[12 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "3108  1409187022338609158  ...  Ceiling fan that receives data on your body he...\n",
            "3109  1409186490823806982  ...          @Sothebys “After” https://t.co/j41a59Gxyw\n",
            "3110  1409184820517687296  ...                              @inversepomp A+ tweet\n",
            "3111  1408934517339951105  ...  @PatrickWStanley Mom’s spaghetti, hands are st...\n",
            "3112  1408862131424927746  ...                              Weekends = Weak Hands\n",
            "...                   ...  ...                                                ...\n",
            "3233  1406991559636111364  ...      @AdamSinger Admittedly, Mar is a fetch quest.\n",
            "3234  1406990216880037890  ...  @AdamSinger Would strongly prefer to stay. But...\n",
            "3235  1406989869876879360  ...  Lightweight, haptic space suit where the helme...\n",
            "3236  1406982506373521408  ...                             Classic dump and dump.\n",
            "3237  1406796930202566662  ...  @extragramken @BoredApeYC This makes me very h...\n",
            "\n",
            "[130 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "2992  1411709535035695110  ...                         @NotRealAI Please DM info.\n",
            "2993  1411378957493018630  ...  @TurnerNovak @Zecca_Lehn @WillManidis @PartyRo...\n",
            "2994  1411321986094043143  ...  7) This is a preview of the type of emerging p...\n",
            "2995  1411164341903130627  ...                             @JRArtSpace_NFT Wow!!!\n",
            "2996  1411120177652002827  ...  RT @BoredElonMusk: Fireworks are silly. There'...\n",
            "...                   ...  ...                                                ...\n",
            "3103  1409714046098542610  ...                                  @kyokill_ Badass.\n",
            "3104  1409630703180861445  ...                        I was born August 12, 2013.\n",
            "3105  1409629850021687299  ...      @PeterMcGraw My birthday is August 12, 2013 🤓\n",
            "3106  1409554888695029772  ...  @packyM Congrats on entering the “I’m going to...\n",
            "3107  1409367300302536707  ...  Aerodynamic birthday cake that allows you to b...\n",
            "\n",
            "[116 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "2923  1414058439802687488  ...                           @fvckrender @richerd Yep\n",
            "2924  1413923180872232961  ...  @balajis @wheatpond Good point. Anon is dispos...\n",
            "2925  1413907922417897472  ...  It’s good to see this trend continue and I aim...\n",
            "2926  1413854495755231233  ...  @greg16676935420 Good morning or evening to ev...\n",
            "2927  1413853978027204611  ...  @VCComplaints @BowditchBoy You can resell it a...\n",
            "...                   ...  ...                                                ...\n",
            "2987  1412431352603963393  ...             My new butler. https://t.co/ItjJwXpn0B\n",
            "2988  1412409264136728577  ...  @ricgalbraith @jackbutcher Duke? I’m not going...\n",
            "2989  1412406456901464064  ...  Yield &gt; taxes. The relationship between cit...\n",
            "2990  1412099253451694083  ...       @apeinacoupe @ferrugenfish @The_Ashen_Fox 🙏👋\n",
            "2991  1411843503571697664  ...  There’s nothing more American on 4th of July t...\n",
            "\n",
            "[69 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "2845  1416903337492897793  ...  Genetically modified popcorn that’s just one g...\n",
            "2846  1416643922957258752  ...  Plates with cutting board material down the mi...\n",
            "2847  1416589614324731904  ...  This is my investment thesis in a tweet. https...\n",
            "2848  1416580493462556674  ...  @mineCityCoins @PatrickWStanley But why male m...\n",
            "2849  1416478033611874304  ...  @balajis Considering how small and powerful th...\n",
            "...                   ...  ...                                                ...\n",
            "2918  1414630234964570118  ...  @Axie44 @AxieAur @elonmusk I don’t currently o...\n",
            "2919  1414622736496828416  ...                 @sean_mcdonald @packyM Your words.\n",
            "2920  1414618987346812933  ...            Just to be clear, it’s not a death ray.\n",
            "2921  1414618562153443330  ...  Giant magnifying glass that is pointed and con...\n",
            "2922  1414588162421129221  ...                                 @packyM Congrats 🔥\n",
            "\n",
            "[78 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "2812  1418971326916792326  ...  A service that books all your wedding vendors ...\n",
            "2813  1418895600154730506  ...  Nice to see the market taking notice. https://...\n",
            "2814  1418637173725679617  ...                                @0xwagmi What's up?\n",
            "2815  1418265986969399297  ...  If companies are experiencing \"high call volum...\n",
            "2816  1418255775005220872  ...                       @PartyRound Mars, April 2013\n",
            "2817  1417956254413033473  ...                    @digitalartchick Great problem!\n",
            "2818  1417909301641842689  ...  Bingo. And as much as people will try...no one...\n",
            "2819  1417906280719536131  ...  We need an emoji for, “these were fun texts bu...\n",
            "2820  1417878514267607041  ...                             @MorningBrew Inflation\n",
            "2821  1417670803269718017  ...              @waitbutwhy @JesseAbraham I knew it!!\n",
            "2822  1417670074937143296  ...                   @danheld https://t.co/0gRPaRZqa8\n",
            "2823  1417650232360202241  ...  @_darrenbacon Thank you for understanding how ...\n",
            "2824  1417628887110930436  ...  I am disappointed with the cheap shots taken a...\n",
            "2825  1417625705412579329  ...            @shl @garrytan Finally someone gets it.\n",
            "2826  1417605106078568449  ...  What a historic day to see Bitcoin under $30k ...\n",
            "2827  1417511243221323784  ...  People are starving all over the world and ins...\n",
            "2828  1417357658231697410  ...  I find it extremely weird that @twitter does n...\n",
            "2829  1417343830676836353  ...  @TrungTPhan This looks like a glitch in Grand ...\n",
            "2830  1417330122932117507  ...                @lay2000lbs https://t.co/GGRYrNUXr3\n",
            "2831  1417315966526967809  ...                          @RaoulGMI Good interview.\n",
            "2832  1417290393889116160  ...  Conference that doesn’t allow participants to ...\n",
            "2833  1417286335405072560  ...  @LoganRyanGolema @sriramk An enterprising crui...\n",
            "2834  1417277896276680706  ...  RT @BoredElonMusk: @sriramk A cruise ship that...\n",
            "2835  1417267733163515938  ...  @RyanSAdams Can you imagine showing a check to...\n",
            "2836  1417267599260323840  ...  @sriramk A cruise ship that travels around the...\n",
            "2837  1417223434120007693  ...  @antoniogm Sure there are lots of issues, but ...\n",
            "2838  1417174819007696896  ...  @thenerdgang I would argue that more often tha...\n",
            "2839  1417174597523283973  ...  @balajis Is there a chart like this for “digit...\n",
            "2840  1417172408583086080  ...            @kendallhtucker https://t.co/1rMWSNusBS\n",
            "2841  1417168674637553684  ...              Guilt is not a good marketing tactic.\n",
            "2842  1417126967044829189  ...  It's incredible what you can get paid to do wi...\n",
            "2843  1417120283459129354  ...  A projectile GPS tracking device that fires in...\n",
            "2844  1416932315796676610  ...  Non-profit gyms designed for human generated e...\n",
            "\n",
            "[33 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "2784  1421909644709044225  ...                                     @nftdao Go on.\n",
            "2785  1421889416340000773  ...                  “Entropy” https://t.co/pmg6S4SzMU\n",
            "2786  1421873530308677633  ...  @adammazurick @elonmusk Is this a serious ques...\n",
            "2787  1421671417389588484  ...  We will begin fielding an Olympic team to repr...\n",
            "2788  1421492491115581448  ...  @bariweiss @DavidSacks @PayPal Can’t be evil &...\n",
            "2789  1421472360352292871  ...  @packyM I don’t have time to reconfigure my te...\n",
            "2790  1421471644019658753  ...  @packyM @sweatystartup I couldn’t even get a f...\n",
            "2791  1421305483130933249  ...                       @MetafiProtocol can we chat?\n",
            "2792  1421259383510429703  ...  Cars that won’t exceed posted speed limits if ...\n",
            "2793  1421122727105110017  ...                @RyanSAdams https://t.co/77wTFEkwmk\n",
            "2794  1421118386642165769  ...          Idea: Every Friday, assume YOU are wrong.\n",
            "2795  1421038532022325252  ...           @TheSmarmyBum @EpicGames @Roblox Correct\n",
            "2796  1420917137971974148  ...                        @Keith_Wasserman Holy shit.\n",
            "2797  1420893646283411457  ...  A version of Twitter where two things can be t...\n",
            "2798  1420842496389632001  ...                             @DonaldMustard Oh hey.\n",
            "2799  1420841772696031234  ...  @TrungTPhan @jackbutcher @niadesk Trung I'm on...\n",
            "2800  1420836719591497729  ...  @TheSmarmyBum @gmoneyNFT I’ve been playing tha...\n",
            "2801  1420804768717426691  ...               @TurnerNovak https://t.co/YAATthpDqb\n",
            "2802  1420757187207471116  ...             A cyberpunk strip club called Laptops.\n",
            "2803  1420738077388021761  ...                              Making stuff is hard.\n",
            "2804  1420549572917485573  ...            @TikTokInvestors @Team3D_Official Soon.\n",
            "2805  1420530943337529346  ...                        @ARKloster Send term sheet.\n",
            "2806  1420474522738720770  ...  Let’s talk about building and rebuilding citie...\n",
            "2807  1420439734501789697  ...                 Shadowy Super Coders as a Service.\n",
            "2808  1420398998645469187  ...  Do not fault people for believing something, b...\n",
            "2809  1420381905275416578  ...  The best questions tend to have the worst answ...\n",
            "2810  1420051760735870980  ...      @greg16676935420 pictures or it didn't happen\n",
            "2811  1420047696652505090  ...  Sunglasses that play a little rock n’ roll whe...\n",
            "\n",
            "[28 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "2692  1424516918682152960  ...  @CatPuts @pownft They will cost about .18 ETH ...\n",
            "2693  1424507842405437441  ...  @tprstly I think that is the only outcome in w...\n",
            "2694  1424497569212731392  ...          @Voodootantra @pownft It’s worth the time\n",
            "2695  1424495560170434563  ...  Many buyers of NFT collections get frustrated ...\n",
            "2696  1424485925455761408  ...  @Struneberg @pownft One of my favorite parts a...\n",
            "...                   ...  ...                                                ...\n",
            "2779  1422214833341014019  ...                      @jack https://t.co/BI6bRz4S2A\n",
            "2780  1422214413013053441  ...  @nnimrodd @ThisIsAito @33NFT I have never boug...\n",
            "2781  1422210311432466434  ...  An Olympic-level literal sport around the conc...\n",
            "2782  1421991859782488069  ...  @elonmusk Relevant to the neon sign ya got the...\n",
            "2783  1421989182633758727  ...                    Gyroscopic couch drink coaster.\n",
            "\n",
            "[92 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "2601  1427055777580150787  ...                         @fvckrender Congrats dude.\n",
            "2602  1427055187722608640  ...  @pranksyNFT @PhillyETH @beaniemaxi @pixelvault...\n",
            "2603  1427047982294126595  ...  @nitikamusing @MetaMask @rainbowdotme Not fami...\n",
            "2604  1427046743615565826  ...  People don’t need to learn to code, they just ...\n",
            "2605  1426918192811626500  ...  @TrungTPhan Moon watch. I know…so on brand. ht...\n",
            "...                   ...  ...                                                ...\n",
            "2687  1424740185749606408  ...  @Keith_Wasserman You can live in an apartment,...\n",
            "2688  1424739840738750466  ...  Service that lets you hire someone to read/rec...\n",
            "2689  1424734302768558082  ...  @packyM @eugenewei Are you planning on an audi...\n",
            "2690  1424733539510067202  ...       @packyM @eugenewei “ThinkBoi” - I feel seen.\n",
            "2691  1424727463125426183  ...  @RyanSAdams Was that second to last word a pur...\n",
            "\n",
            "[91 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "2461  1429554101604343811  ...  @lalotrage @NathanHeadPhoto @rarible @opensea ...\n",
            "2462  1429495420338663438  ...  @NathanHeadPhoto Has anyone done a \"trade-in\" ...\n",
            "2463  1429466133199921152  ...  Fairly sure lobsters will survive on Mars just...\n",
            "2464  1429404463391936515  ...  @amytongwu In some ways, Axie is just proof of...\n",
            "2465  1429399909317316610  ...  @amytongwu My opinion is that the future for g...\n",
            "...                   ...  ...                                                ...\n",
            "2596  1427288161504284679  ...  This will be an opportunity to turn code into ...\n",
            "2597  1427285527963729922  ...             @mineCityCoins https://t.co/Sffh2BUoKA\n",
            "2598  1427245116503203841  ...  @Fitchinverse @pownft Couldn’t agree more. Und...\n",
            "2599  1427079018990374915  ...  Mars will offer mutually assured destruction a...\n",
            "2600  1427073103788605440  ...  @dougboneparth @BenjaminTseng Seriously. Twitt...\n",
            "\n",
            "[140 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "2367  1432077927269556224  ...  @BAYCBoshi @BoredApe1974 @BoredApeYC Let’s do ...\n",
            "2368  1431830774353829888  ...                  @iamwesselart @cryptovoxels I do!\n",
            "2369  1431724672790401026  ...        @Syncubate @TimBeiko @pownft Happy to help.\n",
            "2370  1431661156897210372  ...                          @baugasm @playingarts Me!\n",
            "2371  1431646381609938946  ...            @boredgentleman Just for the weekend :)\n",
            "...                   ...  ...                                                ...\n",
            "2456  1429838275766743046  ...           @BT Looking forward to seeing the magic.\n",
            "2457  1429823579609206785  ...                                   Derisk yourself.\n",
            "2458  1429814338752184320  ...  @michaelxbloch I will smash the unfollow butto...\n",
            "2459  1429811939811553283  ...  @packyM @michaelxbloch @SBF_FTX @a16z I demand...\n",
            "2460  1429670705889566722  ...  @StaniKulechov Have a listen to music generate...\n",
            "\n",
            "[94 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "2116  1434663376274145280  ...  People who can actually give you financial adv...\n",
            "2117  1434639760354516998  ...  @AdamSinger It’s odd they haven’t taken the VC...\n",
            "2118  1434639175362355203  ...  Especially when that IP makes every investor/o...\n",
            "2119  1434638970395103237  ...  If attention is the scarcest resource in the m...\n",
            "2120  1434638398648557570  ...  Creating a brand new IP in a world of Marvel, ...\n",
            "...                   ...  ...                                                ...\n",
            "2362  1432355282160472064  ...  If you think misleading video editing is bad n...\n",
            "2363  1432330120811843585  ...                                   @packyM Web3===D\n",
            "2364  1432197339024789507  ...  @TheeHustleHouse @CapetainTrippy @BoredApeYC Y...\n",
            "2365  1432170826636267522  ...  @jmt_nft @BoredApeYC Solid thought process. I ...\n",
            "2366  1432135023835029507  ...  It the metaverse doesn’t have bathrooms, how w...\n",
            "\n",
            "[251 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "1951  1437202908055883781  ...      @SabaMakesNFTs @digitalartchick More of this.\n",
            "1952  1437198231465725953  ...                               @balajis Thankfully.\n",
            "1953  1437184681435230208  ...                            @pmg Just go with it :)\n",
            "1954  1437183753772625923  ...  An exterior electromagnetic doorknob that attr...\n",
            "1955  1437181581173481474  ...                 @gmoneyNFT @LordTylerWard #Stolana\n",
            "...                   ...  ...                                                ...\n",
            "2111  1434696810614837252  ...  @DaseinScience Well I'm going to make it an ac...\n",
            "2112  1434695535777116164  ...  @jw This business is for things we need to dis...\n",
            "2113  1434695197313548289  ...        Launching things into the sun as a service.\n",
            "2114  1434693943891881993  ...  @Loopifyyy It's no stranger than any luxury br...\n",
            "2115  1434679373295091712  ...                        @levie Lost: Silicon Valley\n",
            "\n",
            "[165 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "1789  1439733535925956609  ...  @jasonkeath @jack I don't think accumulating w...\n",
            "1790  1439732957556588552  ...  @kfury @jack I actually think that's more like...\n",
            "1791  1439730641386434560  ...  Prediction: @Jack is accumulating Bitcoin so t...\n",
            "1792  1439726625000603651  ...  @DangerWillRobin Turning a dystopian city into...\n",
            "1793  1439723027709583361  ...  @DangerWillRobin @rob_omatic Ya Utopia is too ...\n",
            "...                   ...  ...                                                ...\n",
            "1946  1437265970993586179  ...  @austin_rief @packyM It cost me $24 in gas to ...\n",
            "1947  1437265494822699014  ...               @austin_rief https://t.co/bi3SzqDNVq\n",
            "1948  1437255047453155329  ...                                @friesframe Stolana\n",
            "1949  1437253610778230789  ...  @sayinshallah I make that every 3 hours with m...\n",
            "1950  1437243192806502403  ...  @nftbuzz_eth @tryShowtime Thanks for the mention.\n",
            "\n",
            "[162 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "1649  1442209424966258691  ...  @alexisohanian @colleenklein @SorareHQ @AxieIn...\n",
            "1650  1442172865105727489  ...              @pranksy @nftboxes @sad_girls_bar 🔥🔥🔥\n",
            "1651  1442089196504502276  ...  @PimDeWitte This seems painfully obvious to an...\n",
            "1652  1442084626785210370  ...                   @ElijahCTG @VoxiesNFT Good call.\n",
            "1653  1442081578088157190  ...  @balajis Perhaps more efficiency can be achiev...\n",
            "...                   ...  ...                                                ...\n",
            "1784  1439828002548826113  ...                     @0x_b1 https://t.co/QJtVVXJXBD\n",
            "1785  1439817037392465926  ...                            https://t.co/HHIATJPqo9\n",
            "1786  1439806148027187201  ...                                  @eip1559 @jack Ok\n",
            "1787  1439778975836868610  ...                   @WakingCosmos @jack Great minds.\n",
            "1788  1439770098785402881  ...  @WakaFlocka @chefgmi There's a great generativ...\n",
            "\n",
            "[140 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "1461  1444807276594819073  ...           @Ethermorelore @DangerWillRobin Exactly.\n",
            "1462  1444732020257529859  ...                     @BT @crypto888crypto Congrats!\n",
            "1463  1444731960585166849  ...  When a project doesn't sell out in 30 minutes,...\n",
            "1464  1444702740379172871  ...  @raihan_ @FWBtweets @derekgtaylor @drewcoffman...\n",
            "1465  1444677295537459203  ...  @tomfgoodwin Indeed they do not. One is a syst...\n",
            "...                   ...  ...                                                ...\n",
            "1644  1442462775515643907  ...  @farokh @JordiMoraArt Respectful push back on ...\n",
            "1645  1442436995435806726  ...  @naval @robert_w1987 @cdixon It’s called “w3b”...\n",
            "1646  1442324365777932291  ...  @gregisenberg I honestly believe it's a trilli...\n",
            "1647  1442322107149422602  ...              @gregisenberg https://t.co/GVPUR17LHJ\n",
            "1648  1442321872037683202  ...      @awrigh01 If at first you can't succeed, sue.\n",
            "\n",
            "[188 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "1324  1447350157859643392  ...  @Cooopahtroopa Great reminder. Shower thoughts...\n",
            "1325  1447334656051867648  ...                @msenese Have fun staying exploded.\n",
            "1326  1447305083033567233  ...                                    @punk2476 Same.\n",
            "1327  1447304943501672449  ...  Do not eat an \"everything\" bagel inside a spac...\n",
            "1328  1447134566813032449  ...             @mineCityCoins https://t.co/AL4avsEPHK\n",
            "...                   ...  ...                                                ...\n",
            "1456  1445082715724779522  ...  @thenftattorney Yeah. The answer was always gi...\n",
            "1457  1445082439634722819  ...  @ricgalbraith I would equate Web3 more to Lewi...\n",
            "1458  1445081689303052290  ...  Decentralized social media will be a wild west...\n",
            "1459  1445081198481461251  ...                                       @jack Savage\n",
            "1460  1445058845558992898  ...  My Facebook account is down every day. And wil...\n",
            "\n",
            "[137 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "1264  1449577849371901959  ...                         @Grimezsz This is the way.\n",
            "1265  1449272515734372355  ...  How inflation works when prices don’t go up:\\n...\n",
            "1266  1449244894443229184  ...  @TheSmarmyBum @pranksy @Apple Which is an odd ...\n",
            "1267  1449237989012410369  ...  @pranksy @Apple That is a component. Additiona...\n",
            "1268  1449223179768107009  ...  @TimSweeneyEpic Thank you Tim. This is how to ...\n",
            "1269  1449221549194678273  ...  If you’re a blockchain game developer who has ...\n",
            "1270  1449216484757147654  ...  @AbdoEleish @Steam @GoGalaGames Yep very likel...\n",
            "1271  1449159942045319171  ...  @lisztmachina @Steam I think so too. I think t...\n",
            "1272  1449158905599303690  ...  We can do this with you or without you. Person...\n",
            "1273  1449141774677577731  ...                                @HanYoloNFT Boomers\n",
            "1274  1448786795219415068  ...  Yes. Key word is “adults.” https://t.co/j5DQpR...\n",
            "1275  1448778236440113161  ...          @ToadSageNaruto Model Q dies sound good….\n",
            "1276  1448776760636555266  ...                                  @abu3mair Boooooo\n",
            "1277  1448773046924242947  ...  If I designed James Bond’s car you wouldn’t ne...\n",
            "1278  1448742521551548422  ...                             @NathanHeadPhoto Ditto\n",
            "1279  1448742095225720832  ...                   @ApeGuevara I like that a latke.\n",
            "1280  1448690323928981513  ...  I have a collection of 10,000 mechs. They have...\n",
            "1281  1448525324199727106  ...  Police helicopters are the leaf blowers of the...\n",
            "1282  1448524924079861768  ...                     @brian_armstrong Question 🙋‍♂️\n",
            "1283  1448523417745977344  ...  @pt @jonaslamis This is why I’ve shifted almos...\n",
            "1284  1448516009468432385  ...  @synccreationn @Ulkamak1 @maxcapacity This is ...\n",
            "1285  1448436167028789248  ...  @packyM Ok what is this all about? Totally mis...\n",
            "1286  1448398145545670658  ...  @businessbarista @TrungTPhan Hey I private mes...\n",
            "1287  1448390532745490432  ...                                  01100111 01101101\n",
            "1288  1448386998759084035  ...           @eddiegangland It’s probably something 😁\n",
            "1289  1448329958523162628  ...                @bit1b0 @deekaymotion I am in love.\n",
            "1290  1448328000404000772  ...                 @deekaymotion Woah! What is this??\n",
            "1291  1448096413745696771  ...                     @mdudas @LeagueDAO Super cool.\n",
            "1292  1448065677483405312  ...  @mikealfred Something something that’s also a ...\n",
            "1293  1448065492866920448  ...  @mikealfred Something something that’s also a ...\n",
            "1294  1448064395272019969  ...  @mikealfred The Chief Meme Officer role was in...\n",
            "1295  1448049432721719296  ...  I want to secretly travel to the Mars and bury...\n",
            "1296  1448046010454872066  ...  @AlexDaUkrainian Borders are one version of bo...\n",
            "1297  1447969977684750340  ...  @AskTinNguyen The pleasure was mine. Honestly ...\n",
            "1298  1447966462635745282  ...  @CoinbaseWallet Will you be adding support for...\n",
            "1299  1447956222930345988  ...     @gponcin @stripe This seems like a good thing.\n",
            "1300  1447947655028506628  ...      @RyanNegri This is what inflation looks like.\n",
            "1301  1447941076111675392  ...  Your company is not experiencing a high volume...\n",
            "1302  1447785253523714048  ...      @ZawarQayyum Thanks! Got some help already :)\n",
            "1303  1447747722904170507  ...                             @ohihello1 Oh hi hello\n",
            "1304  1447733559922356224  ...                          @jackmallers @Twitter 😘🤝🔥\n",
            "1305  1447733209563684867  ...                @jackmallers @Twitter You’re crazy.\n",
            "1306  1447724972181315587  ...  @sayinshallah @OlympusDAO DAO participants are...\n",
            "1307  1447722537819930624  ...  @aaronmcdnz @altstatemachine https://t.co/RrVw...\n",
            "1308  1447721137522180097  ...     @businessbarista Don’t spoil my trade secret 😂\n",
            "1309  1447719224894365697  ...  A Twitter “out of office” AI bot that scrapes ...\n",
            "1310  1447712455468011521  ...  @makosloff @ODDeepEnd @beondeck @balajis I sha...\n",
            "1311  1447712153033588741  ...        @jasondchap @balajis @makosloff JPEG JOCKEY\n",
            "1312  1447691424082792448  ...   @DharmaNFT @DeepEndPodcast Thanks for listening.\n",
            "1313  1447681329479311362  ...  Anonymity = No ID No Reputation\\nPseudonymity ...\n",
            "1314  1447613456698331142  ...  @ReeseW Definitely check out @sad_girls_bar by...\n",
            "1315  1447604452395917312  ...                  @opiumhum @ourZORA Great upgrade!\n",
            "1316  1447576630524739603  ...  @mikealfred @mdudas Oh man. I love talking Usu...\n",
            "1317  1447545741950402565  ...  @Loopifyyy Anonymous = No ID no reputation. \\n...\n",
            "1318  1447531189296775171  ...           @nateliason @OlympusDAO Excellent piece.\n",
            "1319  1447372344113070084  ...                               @jackbutcher Loooove\n",
            "1320  1447360822418624513  ...  @muneeb @bogachev_anton @adam3us @Stacks This ...\n",
            "1321  1447359789281918977  ...                                   @AskTinNguyen GN\n",
            "1322  1447358792270712833  ...                   @AskTinNguyen Great neon lights.\n",
            "1323  1447357337702780930  ...    @DeFi_0x It’s a Ponzi. \\n\\n(Buys another 1000…)\n",
            "\n",
            "[60 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "1245  1452216653803053065  ...  Hierarchy of Democracy\\n\\nVote with your vote....\n",
            "1246  1452212987327041542  ...  @SIPHERxyz @opensea Sneak preview I’m not sell...\n",
            "1247  1451945990995468290  ...  @amytongwu The winners in blockchain gaming wi...\n",
            "1248  1451915807236104192  ...  Can’t wait to play @SIPHERxyz and see this uni...\n",
            "1249  1451622039480160262  ...                 @hunterorrell @thelandvault Yessir\n",
            "1250  1451149417826775047  ...  @DangerWillRobin @Laxing @TheomanTV @iislim_ji...\n",
            "1251  1451054197294592002  ...  It’s never been easier to get a new location-a...\n",
            "1252  1451046426306355202  ...  RT @BoredElonMusk: Social network that only le...\n",
            "1253  1450745632784850948  ...  @AxieSpike This is a clever way to signal YOU ...\n",
            "1254  1450653589685288965  ...  @iamDCinvestor 1) Your family and friends are ...\n",
            "1255  1450501109324435464  ...  @jonjyan @brianjcho @jasonoliver Done. Keep th...\n",
            "1256  1450499574594953218  ...                          @punk2476 Happy Halloween\n",
            "1257  1450425840718815234  ...  @jayfromp @chriscantino This is a massive vari...\n",
            "1258  1450362269444059140  ...               @farokh 🌕🌗🌒🌓 https://t.co/qG4AaLmcKl\n",
            "1259  1450345732414783488  ...  @LukePlunkett There is a legitimate path to ga...\n",
            "1260  1450337363683643393  ...  All photos on the internet that don’t have ver...\n",
            "1261  1450301487192608770  ...                          @DangerWillRobin @jw GM 👆\n",
            "1262  1450003202364133376  ...  @TheBakaArts The laugh of Sweet Tooth is etche...\n",
            "1263  1450000845014249473  ...           @TheBakaArts Reminds me of Twisted Metal\n",
            "\n",
            "[19 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "1084  1454861758191964162  ...                  @amytongwu Love, Death and Robots\n",
            "1085  1454860114901364738  ...                       @amytongwu Definitely agree.\n",
            "1086  1454836625632944136  ...  A true maximalist should be against any person...\n",
            "1087  1454777745519439878  ...  @balajis @Noahpinion Fair. So the question is ...\n",
            "1088  1454775296524128258  ...  @balajis @Noahpinion I suppose you could use t...\n",
            "...                   ...  ...                                                ...\n",
            "1240  1452873421549867009  ...  @punk6529 The utility of being able to live in...\n",
            "1241  1452785322115297281  ...                        @elonmusk Grudgingly agree.\n",
            "1242  1452516081260769283  ...  Suitcase with built in scale for items inside ...\n",
            "1243  1452455869271056387  ...             @peteskomoroch https://t.co/1nAbKHpOXe\n",
            "1244  1452436952574304256  ...  Try out the demo and let me know what you thin...\n",
            "\n",
            "[161 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                  Tweet_id  ...                                         Tweet_Text\n",
            "963   1457421173822656512  ...  @themariokarters @Slick_NFT I’m going to keep ...\n",
            "964   1457413700185321475  ...  @punk6529 @citizen2890 Fair point. I do apprec...\n",
            "965   1457411211369218049  ...  @citizen2890 @punk6529 How do we overcome the ...\n",
            "966   1457409209251688453  ...  @mineCityCoins @balajis If more competition yi...\n",
            "967   1457383153597702150  ...  RT @BoredElonMusk: Agreeing with someone on on...\n",
            "...                   ...  ...                                                ...\n",
            "1079  1455176985198489604  ...  @tferriss @naval @cdixon Glad you mentioned ga...\n",
            "1080  1455075679528112129  ...  @VitalikButerin Great to see you highlight @mi...\n",
            "1081  1455071791055982592  ...  Ok I figured out how to solve World hunger wit...\n",
            "1082  1454997860370444288  ...  @apaley13 So…you’re not going to accept my fri...\n",
            "1083  1454977689077510144  ...  Your status in this world is not measured by w...\n",
            "\n",
            "[121 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                 Tweet_id  ...                                         Tweet_Text\n",
            "838  1459975282387918849  ...  @PatrickWStanley @cameron @FrancisSuarez @eric...\n",
            "839  1459721089563594753  ...  @JTheAccountant In the business of accounting?...\n",
            "840  1459710142694375427  ...                              @mikealfred I do not.\n",
            "841  1459700890932166660  ...  Clicking links in your DMs… https://t.co/hLuxt...\n",
            "842  1459631450999775235  ...   @MGoPatio I’m not taking anything off the table.\n",
            "..                   ...  ...                                                ...\n",
            "958  1457740704495534080  ...  (2/n) Let's skip what this means for the physi...\n",
            "959  1457740358180167693  ...  In 1993, Richard Garfield developed a powerful...\n",
            "960  1457740209223700487  ...        @mineCityCoins @ericadamsfornyc Great news!\n",
            "961  1457708770088288262  ...                      @charl3svii @punk6529 *scarce\n",
            "962  1457706096110411787  ...  @charl3svii @punk6529 This is why I abhor the ...\n",
            "\n",
            "[125 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                 Tweet_id  ...                                         Tweet_Text\n",
            "777  1462564476259950593  ...  Zero sum thinking is the natural reaction peop...\n",
            "778  1462563038192496641  ...  @RaoulGMI @cdixon Zero sum thinking is the nat...\n",
            "779  1462499162956595206  ...        @YaserBi Fair enough. I guess we shall see.\n",
            "780  1462495086156017665  ...  Please help me understand (and I mean this gen...\n",
            "781  1462440828098412545  ...                          @levelsio This hits hard.\n",
            "..                   ...  ...                                                ...\n",
            "833  1460376110785392644  ...                 @ktsaprailis Glad you caught that.\n",
            "834  1460375006848815105  ...  @AdamSinger At least one version of you…Maybe ...\n",
            "835  1460373237682950145  ...  @AdamSinger Life is like a death ray. Enjoy it...\n",
            "836  1460372374453309444  ...  Will be turning Phobos into an Escape Moon. Yo...\n",
            "837  1460315705719410690  ...  @raihan_ @chrislarsc Most things are performan...\n",
            "\n",
            "[61 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                 Tweet_id  ...                                         Tweet_Text\n",
            "729  1465039904816054274  ...  A restaurant that just increases prices instea...\n",
            "730  1465023098235678726  ...  @heyerikaws @SpaceX Looking for some new appar...\n",
            "731  1465014603532750861  ...  @themariokarters @dylanorrelI @Nike Adidas is ...\n",
            "732  1465014180453367811  ...  @benmayorwhite @BoredApeYC @adidas @NicolasRag...\n",
            "733  1465013798184439808  ...                                      @heyerikaws 👋\n",
            "734  1464992932537114625  ...  @WartuII @ishaheen10 @ConcaveFi @OlympusDAO @a...\n",
            "735  1464991730931879938  ...  @ishaheen10 @ConcaveFi I’m still unclear how t...\n",
            "736  1464816961930493963  ...  @punk6529 @sriramk @srijancloud @balajis My ps...\n",
            "737  1464632011230834689  ...                                 @RAC @Poolsuite GM\n",
            "738  1464394218944675840  ...                      @Poolsuite WAGMI to the pool.\n",
            "739  1464380083175493635  ...  @cdixon I certainly value the knowledge $800/l...\n",
            "740  1464368107422580738  ...  @Cooopahtroopa @Jihoz_Axie @Psycheout86 @roham...\n",
            "741  1464355508232491008  ...  @WartuII @Fiskantes You'll be fine. Happy to s...\n",
            "742  1464348588293591044  ...  @Keith_Wasserman Please explain what flavor tr...\n",
            "743  1464321806949515264  ...                                 @TheSmarmyBum 2???\n",
            "744  1464292172962820102  ...  @TheSmarmyBum I was shopping for a new monitor...\n",
            "745  1464081488555634692  ...            @mgsiegler Pre-ordered the SUV version.\n",
            "746  1463937904955183133  ...                   @packyM See you there virtually.\n",
            "747  1463937423499415577  ...  @packyM Pay-per-view stream. Will only cost .0...\n",
            "748  1463893464681115648  ...               @wrongindustry Fake cranberry sauce.\n",
            "749  1463890238422405122  ...  The Thanksgiving table is like a smaller versi...\n",
            "750  1463679084882391043  ...  @burstbloomdecay @SBF_FTX @moonraygame I am. T...\n",
            "751  1463646925308235778  ...         @RyanNegri It’s just so painfully obvious.\n",
            "752  1463596097620758534  ...                        @butterball Make it happen.\n",
            "753  1463596000677806081  ...  RT @BoredElonMusk: Up to 60% of the human adul...\n",
            "754  1463578445007310849  ...  @julianweisser Absolutely. And so far some pos...\n",
            "755  1463573935107166211  ...  If you liked the principles and goals of @Cons...\n",
            "756  1463562541741662212  ...  @richerd If you haven’t read the Medici Effect...\n",
            "757  1463539038200086534  ...  @socialdiva I feel like a coffee glaze would a...\n",
            "758  1463538290309533696  ...  Caffeinated turkey so you're not sleepy after ...\n",
            "759  1463536990243078149  ...  @tomfgoodwin Just one example...the fact that ...\n",
            "760  1463524142423052298  ...  @tomfgoodwin Are you not tracking the web3 spa...\n",
            "761  1463474299314966534  ...                 @naval But also literally collars.\n",
            "762  1463318147784593408  ...  @cjohndesign @kbclauson This will only end in ...\n",
            "763  1463263563309477888  ...               @thattallguy https://t.co/lhiJseowFj\n",
            "764  1463259240043216896  ...                  @shiraeis https://t.co/I20CcpSHZA\n",
            "765  1463257359245926400  ...  If the only thing you're \"paying\" to an inform...\n",
            "766  1463092064720015368  ...                   @deantak https://t.co/7lcqYRAwGz\n",
            "767  1462948934905520135  ...  @88Gentlegiant @FreshBakedCrate These turned o...\n",
            "768  1462904219204145152  ...  @lightclients @VitalikButerin Thanks for makin...\n",
            "769  1462880267014598660  ...  @RyanSAdams Great concept. Front-end/user expe...\n",
            "770  1462854290704240644  ...            @mysteryoftime @CookieAlchemyst Magical\n",
            "771  1462845305863344132  ...  @Shaughnessy119 @PodcastDelphi @zacharylau @Da...\n",
            "772  1462808673877594115  ...  @packyM @OlympusDAO @ohmzeus @adampatel23 Take...\n",
            "773  1462807590757289985  ...  @packyM Happy to see @OlympusDAO mentioned in ...\n",
            "774  1462766202648809476  ...  @elonmusk @Erdayastronaut @lexfridman Agreed. ...\n",
            "775  1462765608706056198  ...             @lexfridman @alexandre_lores Thank you\n",
            "776  1462762206999633920  ...                              @benedictevans Go on…\n",
            "\n",
            "[48 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                 Tweet_id  ...                                         Tweet_Text\n",
            "630  1467639186115104771  ...  @pixelmagenft Mobile web games are not great. ...\n",
            "631  1467591731990327297  ...  @garrytan Heck yes! I’m rocking only orange Fe...\n",
            "632  1467590126268149763  ...               @brianjcho Orange background is lit.\n",
            "633  1467586714159902723  ...  @Keith_Wasserman And we hate losses more than ...\n",
            "634  1467579936156241922  ...  @PunkHot10 It was a few months ago. I think th...\n",
            "..                   ...  ...                                                ...\n",
            "724  1465356966050480131  ...            @paraga @jack Congrats on the new role.\n",
            "725  1465347139207266312  ...         Phase 2 initiated. https://t.co/nJgzO0eZ60\n",
            "726  1465335970291617795  ...                     @AlmostMedia @philleif @pownft\n",
            "727  1465185167639605257  ...  @HighCoinviction @0xChanglu @altstatemachine S...\n",
            "728  1465170897916940295  ...  @OVioHQ @jamie247 You guys do a fantastic job ...\n",
            "\n",
            "[99 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                 Tweet_id  ...                                         Tweet_Text\n",
            "481  1470173017456013312  ...   @ExileDAO The universe would collapse on itself.\n",
            "482  1470172920152354824  ...  @ishaheen10 @OlympusDAO I also like the symbol...\n",
            "483  1470169945325539330  ...                                @sky8632 Nailed it.\n",
            "484  1470169767751290880  ...  @adampatel23 @WartuII @fattybagz @loss__dot__j...\n",
            "485  1470169610129346561  ...  Forget nuclear or fusion. If we could figure o...\n",
            "..                   ...  ...                                                ...\n",
            "625  1467811275237302272  ...                          @VoxiesNFT Congrats team.\n",
            "626  1467721868685627393  ...  @Shaughnessy119 @PodcastDelphi We were recordi...\n",
            "627  1467676614125187072  ...  @TheSmarmyBum @beaniemaxi @punk4156 The Zune i...\n",
            "628  1467672963625357314  ...  @TheSmarmyBum @beaniemaxi @punk4156 Are you te...\n",
            "629  1467658885896962053  ...   @theaccordance I’d like the freedom to choose :)\n",
            "\n",
            "[149 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                 Tweet_id  ...                                         Tweet_Text\n",
            "368  1472707490768818178  ...  Selling \"scarcity\" in the digital world is a f...\n",
            "369  1472695507742953473  ...     @ILikeNFTs @OlympusDAO https://t.co/YmH1YsYedN\n",
            "370  1472695352104927232  ...                         @princessofdoge But maybe?\n",
            "371  1472694942514307072  ...  @twobitidiot (This is what I tell my employees...\n",
            "372  1472693683707211777  ...  Restaurant booth that scan patrons and display...\n",
            "..                   ...  ...                                                ...\n",
            "476  1470207128421888000  ...  @RedIronRaven @tarmo888 @dfrumps Have you read...\n",
            "477  1470193817475117057  ...                          @WartuII Great job today.\n",
            "478  1470187549775728641  ...  @RedIronRaven @dfrumps The first part I agree ...\n",
            "479  1470182561179582469  ...  @RedIronRaven @dfrumps If we play that game, l...\n",
            "480  1470182018319216641  ...  @galaxybchain @OlympusDAO @opensea Welcome abo...\n",
            "\n",
            "[113 rows x 3 columns]>\n",
            "<bound method DataFrame.count of                 Tweet_id  ...                                         Tweet_Text\n",
            "280  1475246249968168961  ...  @mikethreezy In all seriousness your point is ...\n",
            "281  1475243670248976385  ...               @mikethreezy https://t.co/RC53qHSRcn\n",
            "282  1475238416396607489  ...    @OneBandwagonFan @CryptoNFT_Ave @BoredApeYC Few\n",
            "283  1475187881236832261  ...  @beaniemaxi @avocado_toast2 Work on your deliv...\n",
            "284  1475171389896228871  ...  @TrungTPhan Damn. I still declare you the winn...\n",
            "..                   ...  ...                                                ...\n",
            "363  1472803623507677185  ...                                     @ohhshiny LEGO\n",
            "364  1472786645485162499  ...  @mikethreezy @NftCelestials Remember when Radi...\n",
            "365  1472780348765335553  ...  @mikethreezy @NftCelestials That’s where the p...\n",
            "366  1472773222043045893  ...  @mikethreezy The primary value is authenticity...\n",
            "367  1472751804593049601  ...  @mikealfred For the free individual Monday is ...\n",
            "\n",
            "[88 rows x 3 columns]>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-74ab1bcc-5d55-4e52-95d9-d6de9cb74a20\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet_id</th>\n",
              "      <th>Date</th>\n",
              "      <th>Tweet_Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1478508067213885440</td>\n",
              "      <td>2022-01-04 23:26:07</td>\n",
              "      <td>@jack A lot of energy is spent (wasted) debati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1478504331057852417</td>\n",
              "      <td>2022-01-04 23:11:16</td>\n",
              "      <td>@griillz_eth I'm in.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1478502749281001472</td>\n",
              "      <td>2022-01-04 23:04:59</td>\n",
              "      <td>@mattmireles @john_c_palmer @makinmarkets Ange...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1478455370481766400</td>\n",
              "      <td>2022-01-04 19:56:43</td>\n",
              "      <td>@jamie247 I'd like to learn more but first nee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1478445682075729923</td>\n",
              "      <td>2022-01-04 19:18:13</td>\n",
              "      <td>@brianjcho @amytongwu Cosign on those and if y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3245</th>\n",
              "      <td>1406296934852161549</td>\n",
              "      <td>2021-06-19 17:04:51</td>\n",
              "      <td>@sriramk Welcome</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3246</th>\n",
              "      <td>1406240299442065408</td>\n",
              "      <td>2021-06-19 13:19:48</td>\n",
              "      <td>@fvckrender @Ioannis_AG @Sothebys Congrats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3247</th>\n",
              "      <td>1406226324318945282</td>\n",
              "      <td>2021-06-19 12:24:16</td>\n",
              "      <td>@PatrickWStanley Maybe all the BTC accumulatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3248</th>\n",
              "      <td>1405982338123587588</td>\n",
              "      <td>2021-06-18 20:14:45</td>\n",
              "      <td>@JanetWuNews Optional steam release at the cho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3249</th>\n",
              "      <td>1405977383983669250</td>\n",
              "      <td>2021-06-18 19:55:04</td>\n",
              "      <td>Washer and dryer units that play quiet techno ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3250 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74ab1bcc-5d55-4e52-95d9-d6de9cb74a20')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-74ab1bcc-5d55-4e52-95d9-d6de9cb74a20 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-74ab1bcc-5d55-4e52-95d9-d6de9cb74a20');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 Tweet_id  ...                                         Tweet_Text\n",
              "0     1478508067213885440  ...  @jack A lot of energy is spent (wasted) debati...\n",
              "1     1478504331057852417  ...                               @griillz_eth I'm in.\n",
              "2     1478502749281001472  ...  @mattmireles @john_c_palmer @makinmarkets Ange...\n",
              "3     1478455370481766400  ...  @jamie247 I'd like to learn more but first nee...\n",
              "4     1478445682075729923  ...  @brianjcho @amytongwu Cosign on those and if y...\n",
              "...                   ...  ...                                                ...\n",
              "3245  1406296934852161549  ...                                   @sriramk Welcome\n",
              "3246  1406240299442065408  ...         @fvckrender @Ioannis_AG @Sothebys Congrats\n",
              "3247  1406226324318945282  ...  @PatrickWStanley Maybe all the BTC accumulatio...\n",
              "3248  1405982338123587588  ...  @JanetWuNews Optional steam release at the cho...\n",
              "3249  1405977383983669250  ...  Washer and dryer units that play quiet techno ...\n",
              "\n",
              "[3250 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### DRAFT Collection Object\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "class DataCollection():\n",
        "  def __init__(self, token, output_columns=[\"id\", \"author\", \"post_date\", \"content\"]):\n",
        "        self.token = token\n",
        "        self.output_columns = output_columns\n",
        "  def make_df(self, input_arr):\n",
        "    df = pd.DataFrame(self,input_arr, columns=output_columns)\n",
        "    return df\n",
        "  def help(self):\n",
        "    print(\"This will be a spot to outline the various functions.\")\n",
        "\n",
        "class TwitterCollection(DataCollection):\n",
        "  def get_tweets(self, screen_name, num):\n",
        "    #tweets_list = (screen_name=screen_name, count=num)\n",
        "    req = requests.get('https://api.twitter.com/2/' )\n",
        "    return tweets\n",
        "  \n",
        "class DiscordCollection(DataCollection):\n",
        "\n",
        "  def get_channel_messages(self, channel):\n",
        "    self.headers = {\"Authorization\": f\"Bot {self.token}\"}\n",
        "    channel_path = f'https://discord.com/api/channels/{channel}/messages'\n",
        "    # Optional additional headers\n",
        "    res = requests.get(channel_path, headers=self.headers).json()\n",
        "    for x in res:\n",
        "      print(x)\n",
        "\n",
        "class KaggleCollection(DataCollection):\n",
        "  pass\n",
        "\n",
        "class RedditCollection(DataCollection):\n",
        "  pass\n",
        "\n",
        "class SlackCollection(DataCollection):\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "ziGgbGGxLt76"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testDiscord = DiscordCollection(\"ODg1OTQ1MDUxMzcxNDM0MDI0.YTuaoA.L_qt2Keeils_ZdsdENnzUehG-Hw\")\n",
        "#print(testDiscord.output_columns)\n",
        "print(testDiscord.help())"
      ],
      "metadata": {
        "id": "BjJyFpemeKn2",
        "outputId": "86085cf0-d764-414e-85cc-ea0bb25a3681",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This will be a spot to outline the various functions\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBJECT DESIGN NOTES:\n",
        "\n",
        "Possibilities:\n",
        "\n",
        "* Object takes in a df that has been formatted elsewhere. Object would focus primarily on methods that manipulate the data into different file formats and sorting methods.\n",
        "    * Relies on the user to clean the data and make dataframes that conform to a predtermined column format, which is less magical than the next design\n",
        "    * Follows DRY, SRP, KISS and YAGNI\n",
        "    * Pseudo-code examples:\n",
        "        * `weeks_dataset = new DataSet.weeks(df, filter_by=\"weeks\")`\n",
        "        * `weeks_and_years_dataset = new DataSet.weeks(df, filter_by=['weeks','years'])`\n",
        "\n",
        "* Include methods for scraping the most common datasets, something like:\n",
        "    * Object would attempt to parse out a date object by searching for a column/split point marked as \"date,\" \"created_at\" or similar\n",
        "    * Relies less on end user\n",
        "    * Runs the risk of getting really bloated and trying too hard to cover every edge case for every random data source\n",
        "    * pseudo-code examples:\n",
        "        * `kaggle_data = new DataSet.kaggle(credentials, dataset_name)` (are Kaggle datasets standardized, or are there a ton of different presentations? I think the latter)\n",
        "        * `discord_data = new DataSet.discord(credentials, args* kwargs*)`\n",
        "        * `twittter_data = new DataSet.twitter(< adapt existing function from this notebook>)`"
      ],
      "metadata": {
        "id": "LdOwovr6XHQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TYPE ONE:\n",
        "# This is not exact; it is late at night and mostly an example, I don't remember the exact syntax off the top of my head\n",
        "class CleanDataSet:\n",
        "    def __init__(self, df, file_prefix):\n",
        "        self.input_df = input_df\n",
        "        self.file_prefix = file_prefix\n",
        "        \n",
        "    def parse_by_weeks(self):\n",
        "      for x in range(52):\n",
        "        df = input_df[input_df['Date'].dt.isocalendar().week == x]\n",
        "        print(df.count)\n",
        "        # Only make folders and files for weeks where there were tweets\n",
        "        if not df.empty:\n",
        "          if not os.path.exists(f'week-{x}'):\n",
        "            os.makedirs(f'week-{x}')\n",
        "          df.to_csv(f'week-{x}/{file_prefix}.csv')\n",
        "      return input_df\n",
        "\n",
        "\n",
        "\n",
        "# TYPE TWO:\n",
        "\n",
        "class ScrapeDataSet:\n",
        "    def __init__(self, data_source, file_prefix):\n",
        "        self.data_source = data_source\n",
        "        self.file_prefix = file_prefix\n",
        "        \n",
        "    def kaggle(credentials, dataset):\n",
        "      # Fetch dataset by path (see cell )\n",
        "        "
      ],
      "metadata": {
        "id": "T8l9etSehgl8",
        "outputId": "5fc1fd66-77ea-48c2-9ae4-9f225ca5cd66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-6b6df4e23ae0>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HELPER FUNCTIONS FOR ENRON DATA PARSING\n",
        "\n",
        "import email\n",
        "\n",
        "email_df = pd.read_csv('enron_path') #just change this to the right path and run these two cells\n",
        "\n",
        "def get_text_from_email(msg):\n",
        "    '''To get the content from email objects'''\n",
        "    parts = []\n",
        "    for part in msg.walk():\n",
        "        if part.get_content_type() == 'text/plain':\n",
        "            parts.append( part.get_payload() )\n",
        "    return ''.join(parts)\n",
        "\n",
        "def split_email_addresses(line):\n",
        "    '''To separate multiple email addresses'''\n",
        "    if line:\n",
        "        addrs = line.split(',')\n",
        "        addrs = frozenset(map(lambda x: x.strip(), addrs))\n",
        "    else:\n",
        "        addrs = None\n",
        "    return addrs"
      ],
      "metadata": {
        "id": "4kXDVzJq37QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse the emails into a list email objects\n",
        "messages = list(map(email.message_from_string, emails_df['message']))\n",
        "emails_df.drop('message', axis=1, inplace=True)\n",
        "# Get fields from parsed email objects\n",
        "keys = messages[0].keys()\n",
        "for key in keys:\n",
        "    emails_df[key] = [doc[key] for doc in messages]\n",
        "# Parse content from emails\n",
        "emails_df['content'] = list(map(get_text_from_email, messages))\n",
        "# Split multiple email addresses\n",
        "emails_df['From'] = emails_df['From'].map(split_email_addresses)\n",
        "emails_df['To'] = emails_df['To'].map(split_email_addresses)\n",
        "\n",
        "# Extract the root of 'file' as 'user'\n",
        "emails_df['user'] = emails_df['file'].map(lambda x:x.split('/')[0])\n",
        "del messages\n",
        "\n",
        "emails_df.head()"
      ],
      "metadata": {
        "id": "8nJzypbR4eDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get topics using empath\n",
        "\n",
        "#!pip install empath\n",
        "\n",
        "from empath import Empath\n",
        "lexicon = Empath()\n",
        "\n",
        "#sentence to analyze\n",
        "content = emails_df['content'][2]\n",
        "\n",
        "#analyze\n",
        "nlp = lexicon.analyze(content, normalize=True)\n",
        "\n",
        "#print the content\n",
        "print(content)\n",
        "#print topics with scores greater than 0.5 \n",
        "for key, v in nlp.items():\n",
        "    if v > 0.05:\n",
        "        print(key, v)\n"
      ],
      "metadata": {
        "id": "rqw8pdXL1xmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#join empath results with emails_df where empath results are greater than 0.5\n",
        "#subset emails df to make this run faster for testing\n",
        "emails_df_subset = emails_df[:100]\n",
        "\n",
        "emails_df_subset['empath'] = \"\"\n",
        "for i, row in emails_df_subset.iterrows():\n",
        "    content = row['content']\n",
        "    nlp = lexicon.analyze(content, normalize=True)\n",
        "    for key, v in nlp.items():\n",
        "        if v > 0.05:\n",
        "            emails_df_subset.at[i, 'empath'] = key\n",
        "\n",
        "emails_df_subset['empath'].head()"
      ],
      "metadata": {
        "id": "iX65Kc7C10ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train gpt3 classifier on emapth topcis\n",
        "import openai\n",
        "import config\n",
        "#for gpt2 if needed\n",
        "#import gpt_2_simple as gpt2\n",
        "\n",
        "\n",
        "\n",
        "openai.api_key = config.openai_key\n",
        "\n",
        "#create new df for training\n",
        "train_df = emails_df_subset[['content', 'empath']]\n",
        "#make sure no null values\n",
        "train_df = train_df[train_df['empath'] != \"\"]\n",
        "#im getting errors about the length of the content examples so I need to only select ones that are less than 500 characters till i figure out how to fix this\n",
        "train_df = train_df[train_df['content'].str.len() < 30]\n",
        "\n",
        "#create a dictionary of examples\n",
        "examples_dict = dict(zip(train_df.content, train_df.empath))\n",
        "#for the sake of this test we will reduce the dict down to the first 3 items or gpt3 will error -- will need to train on a csv for more\n",
        "examples_dict = {k: examples_dict[k] for k in list(examples_dict)[:3]}\n",
        "#print(examples_dict)\n",
        "\n",
        "#turn examples into a list of lists for gpt3 formatting\n",
        "examples = list(map(list, examples_dict.items()))\n",
        "\n",
        "#get unique values from the list for the training labels\n",
        "labels = list(set([x for x in examples_dict.values()]))\n",
        "#print(labels)\n",
        "\n",
        "#select random content from emails_df to classify\n",
        "from random import randrange\n",
        "num = randrange(200000)\n",
        "\n",
        "query = emails_df['content'][num]\n",
        "\n",
        "#remove URLS from query\n",
        "import re\n",
        "query = re.sub(r'http\\S+', '', query)\n",
        "\n",
        "#print(query)\n",
        "\n",
        "def gpt_classify(examples, labels, query):\n",
        "  response = openai.Classification.create(\n",
        "        search_model=\"ada\",\n",
        "        model=\"curie\",\n",
        "        examples=[examples],\n",
        "        query=query,\n",
        "        labels=labels,\n",
        "      )\n",
        "  return response.label\n",
        "\n",
        "test = gpt_classify(examples, labels, query)\n",
        "\n",
        "print(test)"
      ],
      "metadata": {
        "id": "MUwG2Hvl13wo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}